{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install tensorflow\n",
    "!pip install image-similarity-measures\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import keras.utils as image\n",
    "import image_similarity_measures\n",
    "from image_similarity_measures.quality_metrics import rmse, ssim, sre\n",
    "from collections import Counter\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"file.zip\"\n",
    "#download the test set\n",
    "if not os.path.exists(\"Test Set/\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1W94tVozlrHMY_SWE-45TvWT5PqQjElxc&confirm=t\", file_path)\n",
    "    gdown.extractall(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "\n",
    "#download the face weights\n",
    "if not os.path.exists(\"Face Weights/\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1xxv9arylSUjjDoe8E5wwwLoMUQLl1bxT&confirm=t\", file_path)\n",
    "    gdown.extractall(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "\n",
    "#Load the Gender Model\n",
    "if not os.path.exists(\"Gender Model/\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1easVnhBN9o1s60_eAyl8CGj_LHuPoXgc&confirm=t\", file_path)\n",
    "    gdown.extractall(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "gender_model = keras.models.load_model('Gender Model/')\n",
    "\n",
    "#Load the Emotion Model\n",
    "if not os.path.exists(\"Emotions Model/\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1zwwyyd0CZZBDOYt6vSliejyq7NrGbRsT&confirm=t\", file_path)\n",
    "    gdown.extractall(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "emotions_model = keras.models.load_model('Emotions Model/')\n",
    "\n",
    "#Download the videos to test\n",
    "if not os.path.exists(\"children.mp4\") or not os.path.exists(\"people.mp4\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1IN-fUHgI3hG0wslmZPkfVS8ZJNGdtvF1&confirm=t\", file_path)\n",
    "    gdown.extractall(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_img_height, emotions_img_height = 64, 64\n",
    "gender_img_width, emotions_img_width = 64, 64\n",
    "gender_class_names = [\"Man\", \"Woman\"]\n",
    "emotions_class_names = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#take an image and return the image ready to be used as input for the net\n",
    "def prepare_img(img):\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    return images\n",
    "\n",
    "#only for live analysis, write over the face detection the most two probable emotions\n",
    "def write_on_box(frame, gender, x, y, h, emotions):\n",
    "    c = 10\n",
    "    cv2.putText(frame, gender, (x, y+h+25), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "    for key, value in emotions.items():\n",
    "        if c < 70:\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                str(key) + \" \" + str(value)[:4], \n",
    "                (x, y-c), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.9, \n",
    "                (36,255,12), \n",
    "                2\n",
    "            )\n",
    "            c += 30\n",
    "\n",
    "#draw a polar char where a point show the emotion, works with multiple person\n",
    "def emotions_chart(emotions, gender, emotions_classes, live, history_dict):\n",
    "    tmp_emotions_class_names = np.insert(emotions_class_names, 6, \"\")\n",
    "    tmp_emotions_classes = np.insert(emotions_classes.flatten(), 6, 0)\n",
    "    fig = plt.figure(figsize=[8, 8])\n",
    "    ax = fig.add_subplot(111, projection='polar')\n",
    "    ax.set_xticklabels(tmp_emotions_class_names)\n",
    "    theta = np.arange( 0, 2*np.pi, 2*np.pi/8)\n",
    "    #if the method is called live\n",
    "    if bool(history_dict):\n",
    "        for label in history_dict.keys():\n",
    "            tmp = history_dict[label][-1]\n",
    "            iterator = iter(tmp)\n",
    "            next(iterator)\n",
    "            key = next(iterator)\n",
    "            i = np.where(tmp_emotions_class_names == key)[0][0]\n",
    "            ax.scatter(theta[i], tmp[key], 100, label=label)\n",
    "            ax.legend()\n",
    "    #if the method is called for an image\n",
    "    else:\n",
    "        key = next(iter(emotions))\n",
    "        i = np.where(tmp_emotions_class_names == key)[0][0]\n",
    "        ax.scatter(theta[i], tmp_emotions_classes[i], 100)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    img  = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "    if live:\n",
    "        cv2.imshow(\"chart\", img)\n",
    "        ax.set_visible(False)\n",
    "        save_history(gender, emotions)\n",
    "\n",
    "def save_history(gender, emotions):\n",
    "    global history\n",
    "    history = np.append(history, {\"Gender\": gender, **emotions})\n",
    "\n",
    "#return the best score fo the similarities\n",
    "def calc_closest_val(similarity, checkMax):\n",
    "    result = {}\n",
    "\n",
    "    if (checkMax):\n",
    "        closest = max(similarity.values())\n",
    "    else:\n",
    "        closest = min(similarity.values())\n",
    "    \n",
    "    for key, value in similarity.items():\n",
    "        if (value == closest):\n",
    "            result[key] = closest\n",
    "            \n",
    "    return result\n",
    "\n",
    "#return the predict gender\n",
    "def find_gender(img):\n",
    "    gender_classes = gender_classes_func(img)\n",
    "    return gender_class_names[np.where(gender_classes > 0.5, 1,0)[0][1]]\n",
    "\n",
    "#return the predict emotions\n",
    "def find_emotions(img):\n",
    "    tmp_dictionary = dict()\n",
    "    emotions = dict()\n",
    "    emotions_classes = emotions_classes_func(img)\n",
    "    index = list(np.where(np.where(emotions_classes > 0, 1, 0)[0] == 1)[0])\n",
    "    for i in index:\n",
    "        tmp_dictionary[emotions_class_names[i]] = emotions_classes[0][i]\n",
    "    sorted_keys = sorted(tmp_dictionary, key=tmp_dictionary.get, reverse=True)\n",
    "    for k in sorted_keys:\n",
    "        emotions[k] = tmp_dictionary[k]*100\n",
    "    return emotions\n",
    "\n",
    "#return all the emotions classes and scores\n",
    "def emotions_classes_func(img):\n",
    "    emotions_img = cv2.resize(np.float32(img), (emotions_img_height, emotions_img_width))\n",
    "    return emotions_model.predict(prepare_img(emotions_img))\n",
    "\n",
    "#return all the gender classes and scores\n",
    "def gender_classes_func(img):\n",
    "    gender_img = cv2.resize(np.float32(img), (gender_img_height, gender_img_width))\n",
    "    return gender_model.predict(prepare_img(gender_img))\n",
    "\n",
    "#check if the face has already been seen on camera and return a score of similarity\n",
    "def calc_similarity(template_faces, face, dim, thr_ssim, thr_sre):\n",
    "    ssim_measures = dict()\n",
    "    rmse_measures = dict()\n",
    "    sre_measures = dict()\n",
    "    for key in template_faces.keys():\n",
    "        resized_img = cv2.resize(template_faces[key], dim, interpolation = cv2.INTER_AREA)\n",
    "        ssim_measures[key]= ssim(face, resized_img)\n",
    "        rmse_measures[key]= rmse(face, resized_img)\n",
    "        sre_measures[key]= sre(face, resized_img)\n",
    "    ssim_dict = calc_closest_val(ssim_measures, True)\n",
    "    rmse_dict = calc_closest_val(rmse_measures, False)\n",
    "    sre_dict = calc_closest_val(sre_measures, True)\n",
    "    key1 = next(iter(ssim_dict))\n",
    "    key2 = next(iter(rmse_dict))\n",
    "    key3 = next(iter(sre_dict))\n",
    "    if ssim_dict[key1] > np.mean(thr_ssim) or sre_dict[key3] > np.mean(thr_sre):\n",
    "        return (Counter([key1,key2,key3]).most_common(1)[0][0], ssim_dict[key1], sre_dict[key3])\n",
    "    else:\n",
    "        return (\"\", ssim_dict[key1], sre_dict[key3])\n",
    "\n",
    "#add a new face to the temporary set\n",
    "def new_person(template_faces, history_dict, faces_index, face, history):\n",
    "    template_faces[f\"Person{faces_index}\"] = face\n",
    "    history_dict[f\"Person{faces_index}\"] = history\n",
    "    faces_index += 1\n",
    "    return (template_faces, history_dict, faces_index)\n",
    "\n",
    "#create a line chart related to the time passed and the emotions\n",
    "def final_chart(history_dict):\n",
    "    for person in history_dict.keys():\n",
    "        single_list = history_dict[person].tolist()\n",
    "        if len(single_list) > 1:\n",
    "            df = pd.DataFrame(single_list)\n",
    "            df = df.drop(columns=\"Gender\")\n",
    "\n",
    "            df.plot(kind='line', stacked=False, figsize = (7,4))\n",
    "            plt.legend(loc=\"center left\", bbox_to_anchor=(1.0,0.5))\n",
    "            plt.xticks(np.arange(len(df)), np.arange(1, len(df)+1), rotation=0)\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Emotions Prediction\")\n",
    "            plt.savefig(f'chart of {person}.png', bbox_inches=\"tight\")\n",
    "        \n",
    "def write_on_text(history_dict):\n",
    "    t = \"\"\n",
    "    for person, emotions in history_dict.items():\n",
    "        if len(emotions) > 1:\n",
    "            t = t + person + \"\\n\"\n",
    "            for emotion in emotions:\n",
    "                for name, prediction in emotion.items():\n",
    "                    t = t + (\"{} ({})\".format(name, prediction)) + \"\\n\"\n",
    "                t = t + \"\\n\"\n",
    "            t = t + \"\\n\"\n",
    "    with open(f'Analysis {datetime.now().strftime(\"%H_%M_%S %d_%m_%Y\")}.txt', 'w') as f:\n",
    "        f.write(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load and Predict an image\n",
    "img = image.load_img(\"Test Set/Gender/woman/100.png\", color_mode = \"grayscale\")\n",
    "face_cascade = cv2.CascadeClassifier('Face Weights/haarcascade_frontalface_alt.xml')\n",
    "faces = face_cascade.detectMultiScale(np.array(img))\n",
    "for (x, y, w, h) in faces:\n",
    "    gender = find_gender(np.array(img)[y:y+h, x:x+w])\n",
    "    emotions = find_emotions(np.array(img)[y:y+h, x:x+w])\n",
    "\n",
    "print(gender + \" \" + str(emotions)[1:-1])\n",
    "\n",
    "plt.imshow(cv2.resize(np.float32(img), (emotions_img_height, emotions_img_width)), cmap=\"gray\")\n",
    "\n",
    "emotions_chart(emotions, gender, emotions_classes_func(img), False, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Video Analyzer\n",
    "videos = [\"children.mp4\", \"people.mp4\"]\n",
    "cap = cv2.VideoCapture(videos[0])\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "skip = 3\n",
    "size = (frame_width,frame_height)\n",
    "if skip == 0:\n",
    "    fps_out = fps\n",
    "else:\n",
    "    fps_out = fps/skip\n",
    "\n",
    "out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'MP4V'), fps_out, size)\n",
    "face_cascade = cv2.CascadeClassifier('Face Weights/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "skip_frames = int(fps_out)\n",
    "cur_frames = 0\n",
    "\n",
    "template_faces = dict()\n",
    "faces_index = 1\n",
    "history_dict = dict()\n",
    "\n",
    "emotions_tmp = {}\n",
    "gender_tmp = {}\n",
    "\n",
    "ssim_dynamic_similarity = [0.85]\n",
    "sre_dynamic_similarity = [50]\n",
    "\n",
    "while True:\n",
    "    for i in range(skip-1):\n",
    "        cap.read()\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        process_frame = cur_frames % skip_frames == 0\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(frame)\n",
    "        for (x, y, w, h) in faces:\n",
    "            if w > 150 and h > 150:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (36,255,12), 2)\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                dim = (int(face.shape[1]),int(face.shape[0]))\n",
    "                if process_frame:\n",
    "                    history = np.array([])\n",
    "                    face_gray = gray[y:y+h, x:x+w]\n",
    "                    \n",
    "                    gender = find_gender(face_gray)\n",
    "                    emotions = find_emotions(face_gray)\n",
    "                    \n",
    "                    write_on_box(frame, gender, x, y, h, emotions)\n",
    "                    save_history(gender, emotions)\n",
    "                    if len(template_faces) > 0:\n",
    "                        key, ssim_val, sre_val = calc_similarity(template_faces, face, dim, ssim_dynamic_similarity, sre_dynamic_similarity)\n",
    "                        ssim_dynamic_similarity.append(ssim_val)\n",
    "                        sre_dynamic_similarity.append(sre_val)\n",
    "                        if key in history_dict:\n",
    "                            history_dict[key] = np.append(history_dict[key], history)\n",
    "                            gender_tmp[key] = gender\n",
    "                            emotions_tmp[key] = emotions\n",
    "                        else:\n",
    "                            template_faces, history_dict, faces_index = new_person(template_faces, history_dict, faces_index, face, history)\n",
    "                            gender_tmp[f\"Person{faces_index-1}\"] = gender\n",
    "                            emotions_tmp[f\"Person{faces_index-1}\"] = emotions\n",
    "                    else:\n",
    "                        template_faces, history_dict, faces_index = new_person(template_faces, history_dict, faces_index, face, history)\n",
    "                        gender_tmp[\"Person1\"] = gender\n",
    "                        emotions_tmp[\"Person1\"] = emotions\n",
    "                else:\n",
    "                    if len(template_faces) > 0:\n",
    "                        key, _, _ = calc_similarity(template_faces, face, dim, ssim_dynamic_similarity, sre_dynamic_similarity)\n",
    "                        if key in gender_tmp:\n",
    "                            write_on_box(frame, gender_tmp[key], x, y, h, emotions_tmp[key])\n",
    "        out.write(frame)\n",
    "        cur_frames += 1\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "write_on_text(history_dict)\n",
    "final_chart(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Live Analyzer\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened(): \n",
    "    print(\"Unable to read camera feed\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "face_cascade = cv2.CascadeClassifier('Face Weights/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "skip_frames = int(fps)/2\n",
    "cur_frames = 0\n",
    "\n",
    "template_faces = dict()\n",
    "faces_index = 1\n",
    "history_dict = dict()\n",
    "\n",
    "emotions_tmp = {}\n",
    "gender_tmp = {}\n",
    "\n",
    "ssim_dynamic_similarity = [0.75]\n",
    "sre_dynamic_similarity = [50]\n",
    "\n",
    "while True:\n",
    "    for i in range(1):\n",
    "        cap.read()\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        process_frame = cur_frames % skip_frames == 0\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(frame)\n",
    "        for (x, y, w, h) in faces:\n",
    "            if w > 100 and h > 100:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (36,255,12), 2)\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "                dim = (int(face.shape[1]),int(face.shape[0]))\n",
    "                if process_frame:\n",
    "                    history = np.array([])\n",
    "                    face_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "                    gender = find_gender(face_gray)\n",
    "                    emotions = find_emotions(face_gray)\n",
    "                    \n",
    "                    write_on_box(frame, gender, x, y, h, emotions)\n",
    "                    emotions_chart(emotions, gender, emotions_classes_func(face_gray), True, history_dict)\n",
    "\n",
    "                    if len(template_faces) > 0:\n",
    "                        key, ssim_val, sre_val = calc_similarity(template_faces, face, dim, ssim_dynamic_similarity, sre_dynamic_similarity)\n",
    "                        ssim_dynamic_similarity.append(ssim_val)\n",
    "                        sre_dynamic_similarity.append(sre_val)\n",
    "                        if key in history_dict:\n",
    "                            history_dict[key] = np.append(history_dict[key], history)\n",
    "                            gender_tmp[key] = gender\n",
    "                            emotions_tmp[key] = emotions\n",
    "                        else:\n",
    "                            template_faces, history_dict, faces_index = new_person(template_faces, history_dict, faces_index, face, history)\n",
    "                            gender_tmp[f\"Person{faces_index-1}\"] = gender\n",
    "                            emotions_tmp[f\"Person{faces_index-1}\"] = emotions\n",
    "                    else:\n",
    "                        template_faces, history_dict, faces_index = new_person(template_faces, history_dict, faces_index, face, history)\n",
    "                        gender_tmp[\"Person1\"] = gender\n",
    "                        emotions_tmp[\"Person1\"] = emotions\n",
    "                else:\n",
    "                    if len(template_faces) > 0:\n",
    "                        key, _, _ = calc_similarity(template_faces, face, dim, ssim_dynamic_similarity, sre_dynamic_similarity)\n",
    "                        if key in gender_tmp:\n",
    "                            write_on_box(frame, gender_tmp[key], x, y, h, emotions_tmp[key])\n",
    "        cv2.imshow('frame', frame)\n",
    "        cur_frames += 1\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "write_on_text(history_dict)\n",
    "final_chart(history_dict)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
