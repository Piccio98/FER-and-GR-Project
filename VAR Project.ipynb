{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python\n",
    "#!pip install tensorflow\n",
    "#!pip install image-similarity-measures\n",
    "\n",
    "import sys\n",
    "import numpy as np \n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import keras.utils as image\n",
    "import image_similarity_measures\n",
    "from image_similarity_measures.quality_metrics import rmse, ssim, sre\n",
    "from collections import Counter\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"test.zip\"\n",
    "#download all the images\n",
    "output = file_path\n",
    "if not os.path.exists(\"Test Set/\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1W94tVozlrHMY_SWE-45TvWT5PqQjElxc\", file_path)\n",
    "    gdown.extractall(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "\n",
    "#Load the Gender Model\n",
    "if not os.path.exists(\"Gender Model/\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1easVnhBN9o1s60_eAyl8CGj_LHuPoXgc\", file_path)\n",
    "    gdown.extractall(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "gender_model = keras.models.load_model('Gender Model/')\n",
    "\n",
    "#Load the Emotion Model\n",
    "if not os.path.exists(\"Emotions Model/\"):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1zwwyyd0CZZBDOYt6vSliejyq7NrGbRsT\", file_path)\n",
    "    gdown.extractall(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "emotions_model = keras.models.load_model('Emotions Model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_img_height = 378\n",
    "gender_img_width = 378\n",
    "emotions_img_height = 224\n",
    "emotions_img_width = 224\n",
    "gender_class_names = [\"Man\", \"Woman\"]\n",
    "emotions_class_names = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def prepare_img(img):\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    return images\n",
    "\n",
    "def write_on_box(frame, gender, x, y, h, emotions):\n",
    "    c = 10\n",
    "    cv2.putText(frame, gender, (x, y+h+25), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "    for key, value in emotions.items():\n",
    "        if c < 70:\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                str(key) + \" \" + str(value)[:4], \n",
    "                (x, y-c), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.9, \n",
    "                (36,255,12), \n",
    "                2\n",
    "            )\n",
    "            c += 30\n",
    "\n",
    "def plotChart(emotions, gender, emotions_classes, live, history_dict):\n",
    "    tmp_emotions_class_names = np.insert(emotions_class_names, 6, \"\")\n",
    "    tmp_emotions_classes = np.insert(emotions_classes.flatten(), 6, 0)\n",
    "    fig = plt.figure(figsize=[8, 8])\n",
    "    ax = fig.add_subplot(111, projection='polar')\n",
    "    ax.set_xticklabels(tmp_emotions_class_names)\n",
    "    theta = np.arange( 0, 2*np.pi, 2*np.pi/8)\n",
    "    if bool(history_dict):\n",
    "        for label in history_dict.keys():\n",
    "            tmp = history_dict[label][-1]\n",
    "            iterator = iter(tmp)\n",
    "            next(iterator)\n",
    "            key = next(iterator)\n",
    "            i = np.where(tmp_emotions_class_names == key)[0][0]\n",
    "            ax.scatter(theta[i], tmp[key], 100, label=label)\n",
    "            ax.legend()\n",
    "    else:\n",
    "        key = next(iter(emotions))\n",
    "        i = np.where(tmp_emotions_class_names == key)[0][0]\n",
    "        ax.scatter(theta[i], tmp_emotions_classes[i], 100)\n",
    "    fig.canvas.draw()\n",
    "    img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    img  = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "    if live:\n",
    "        cv2.imshow(\"chart\", img)\n",
    "        ax.set_visible(False)\n",
    "        save_history(gender, emotions)\n",
    "\n",
    "def save_history(gender, emotions):\n",
    "    global history\n",
    "    history = np.append(history, {\"Gender\": gender, **emotions})\n",
    "\n",
    "def calc_closest_val(similarity, checkMax):\n",
    "    result = {}\n",
    "\n",
    "    if (checkMax):\n",
    "        closest = max(similarity.values())\n",
    "    else:\n",
    "        closest = min(similarity.values())\n",
    "    \n",
    "    for key, value in similarity.items():\n",
    "        if (value == closest):\n",
    "            result[key] = closest\n",
    "            \n",
    "    return result\n",
    "\n",
    "def find_gender(img):\n",
    "    gender_classes = gender_classes_f(img)\n",
    "    return gender_class_names[np.where(gender_classes > 0.5, 1,0)[0][1]]\n",
    "\n",
    "def find_emotions(img):\n",
    "    tmp_dictionary = dict()\n",
    "    emotions = dict()\n",
    "    emotions_classes = emotions_classes_f(img)\n",
    "    index = list(np.where(np.where(emotions_classes > 0, 1, 0)[0] == 1)[0])\n",
    "    for i in index:\n",
    "        tmp_dictionary[emotions_class_names[i]] = emotions_classes[0][i]\n",
    "    sorted_keys = sorted(tmp_dictionary, key=tmp_dictionary.get, reverse=True)\n",
    "    for k in sorted_keys:\n",
    "        emotions[k] = tmp_dictionary[k]*100\n",
    "    return emotions\n",
    "\n",
    "def emotions_classes_f(img):\n",
    "    emotions_img = cv2.resize(np.float32(img), (emotions_img_height, emotions_img_width))\n",
    "    return emotions_model.predict(prepare_img(emotions_img))\n",
    "\n",
    "def gender_classes_f(img):\n",
    "    gender_img = cv2.resize(np.float32(img), (gender_img_height, gender_img_width))\n",
    "    return gender_model.predict(prepare_img(gender_img))\n",
    "\n",
    "def calc_similarity(template_faces, face, dim):\n",
    "    ssim_measures = dict()\n",
    "    rmse_measures = dict()\n",
    "    sre_measures = dict()\n",
    "    for key in template_faces.keys():\n",
    "        resized_img = cv2.resize(template_faces[key], dim, interpolation = cv2.INTER_AREA)\n",
    "        ssim_measures[key]= ssim(face, resized_img)\n",
    "        rmse_measures[key]= rmse(face, resized_img)\n",
    "        sre_measures[key]= sre(face, resized_img)\n",
    "    ssim_dict = calc_closest_val(ssim_measures, True)\n",
    "    rmse_dict = calc_closest_val(rmse_measures, False)\n",
    "    sre_dict = calc_closest_val(sre_measures, True)\n",
    "    key1 = next(iter(ssim_dict))\n",
    "    key2 = next(iter(rmse_dict))\n",
    "    key3 = next(iter(sre_dict))\n",
    "\n",
    "    if ssim_dict[key1] > 0.8 or sre_dict[key3] > 55:\n",
    "        return Counter([key1,key2,key3]).most_common(1)[0][0]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def new_person(template_faces, history_dict, faces_index, face, history):\n",
    "    template_faces[f\"Person{faces_index}\"] = face\n",
    "    history_dict[f\"Person{faces_index}\"] = history\n",
    "    faces_index += 1\n",
    "    return (template_faces, history_dict, faces_index)\n",
    "\n",
    "def chart(history_dict):\n",
    "    for person in history_dict.keys():\n",
    "        df = pd.DataFrame(history_dict[person].tolist())\n",
    "        df = df.drop(columns=\"Gender\")\n",
    "\n",
    "        df.plot(kind='line', stacked=False, figsize = (7,4))\n",
    "        plt.legend(loc=\"center left\", bbox_to_anchor=(1.0,0.5))\n",
    "        plt.xticks(np.arange(len(df)), np.arange(1, len(df)+1), rotation=0)\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Emotions Prediction\")\n",
    "        plt.savefig(f'chart of {person}.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Load and Predict an image\n",
    "img = image.load_img(\"Test Set/Gender/Woman/100.png\", color_mode = \"grayscale\")\n",
    "face_cascade = cv2.CascadeClassifier('Face Weights/haarcascade_frontalface_alt.xml')\n",
    "faces = face_cascade.detectMultiScale(np.array(img))\n",
    "for (x, y, w, h) in faces:\n",
    "    gender = find_gender(np.array(img)[y:y+h, x:x+w])\n",
    "    emotions = find_emotions(np.array(img)[y:y+h, x:x+w])\n",
    "\n",
    "print(gender + \" \" + str(emotions)[1:-1])\n",
    "\n",
    "plt.imshow(cv2.resize(np.float32(img), (emotions_img_height, emotions_img_width)), cmap=\"gray\")\n",
    "\n",
    "plotChart(emotions, gender, emotions_classes_f(img), False, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"test.mp4\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "skip = 3\n",
    "size = (frame_width,frame_height)\n",
    "\n",
    "out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'MP4V'), int(fps/skip), size)\n",
    "face_cascade = cv2.CascadeClassifier('Face Weights/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "skip_frames = int(fps/skip)\n",
    "cur_frames = 0\n",
    "\n",
    "template_faces = dict()\n",
    "faces_index = 1\n",
    "history_dict = dict()\n",
    "\n",
    "while True:\n",
    "    for i in range(skip):\n",
    "        cap.read()\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        process_frame = cur_frames % skip_frames == 0\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(frame)\n",
    "        for (x, y, w, h) in faces:\n",
    "            if w > 100 and h > 100:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (36,255,12), 2)\n",
    "                if process_frame:\n",
    "                    history = np.array([])\n",
    "                    face = frame[y:y+h, x:x+w]\n",
    "                    face_gray = gray[y:y+h, x:x+w]\n",
    "                    dim = (int(face.shape[1]),int(face.shape[0]))\n",
    "                    gender = find_gender(face_gray)\n",
    "                    emotions = find_emotions(face_gray)\n",
    "                    save_history(gender, emotions)\n",
    "                    if len(template_faces) > 0:\n",
    "                        key = calc_similarity(template_faces, face, dim)\n",
    "                        if key in history_dict:\n",
    "                            history_dict[key] = np.append(history_dict[key], history)\n",
    "                        else:\n",
    "                            template_faces, history_dict, faces_index = new_person(template_faces, history_dict, faces_index, face, history)\n",
    "                    else:\n",
    "                        template_faces, history_dict, faces_index = new_person(template_faces, history_dict, faces_index, face, history)\n",
    "                else:\n",
    "                    write_on_box(frame, gender, x, y, h, emotions)\n",
    "        out.write(frame)\n",
    "        cur_frames += 1\n",
    "    else:\n",
    "        break\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "with open(f'Analysis {datetime.now().strftime(\"%H_%M_%S %d_%m_%Y\")}.txt', 'w') as f:\n",
    "    f.write(str(history_dict))\n",
    "chart(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened(): \n",
    "    print(\"Unable to read camera feed\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "face_cascade = cv2.CascadeClassifier('Face Weights/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "skip_frames = int(fps)/2\n",
    "cur_frames = 0\n",
    "\n",
    "template_faces = dict()\n",
    "faces_index = 1\n",
    "history_dict = dict()\n",
    "\n",
    "while True:\n",
    "    for i in range(1):\n",
    "        cap.read()\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        process_frame = cur_frames % skip_frames == 0\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(frame)\n",
    "        for (x, y, w, h) in faces:\n",
    "            if w > 100 and h > 100:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (36,255,12), 2)\n",
    "                if process_frame:\n",
    "                    history = np.array([])\n",
    "                    face = frame[y:y+h, x:x+w]\n",
    "                    face_gray = gray[y:y+h, x:x+w]\n",
    "                    dim = (int(face.shape[1]),int(face.shape[0]))\n",
    "\n",
    "                    gender = find_gender(face_gray)\n",
    "                    emotions = find_emotions(face_gray)\n",
    "\n",
    "                    write_on_box(frame, gender, x, y, h, emotions)\n",
    "                    plotChart(emotions, gender, emotions_classes_f(face_gray), True, history_dict)\n",
    "\n",
    "                    if len(template_faces) > 0:\n",
    "                        key = calc_similarity(template_faces, face, dim)\n",
    "                        if key in history_dict:\n",
    "                             history_dict[key] = np.append(history_dict[key], history)\n",
    "                        else:\n",
    "                            template_faces, history_dict, faces_index = new_person(template_faces, history_dict, faces_index, face, history)\n",
    "                    else:\n",
    "                        template_faces, history_dict, faces_index = new_person(template_faces, history_dict, faces_index, face, history)\n",
    "                else:\n",
    "                    write_on_box(frame, gender, x, y, h, emotions)\n",
    "        cv2.imshow('frame', frame)\n",
    "        cur_frames += 1\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "with open(f'Analysis {datetime.now().strftime(\"%H_%M_%S %d_%m_%Y\")}.txt', 'w') as f:\n",
    "    f.write(str(history_dict))\n",
    "chart(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
